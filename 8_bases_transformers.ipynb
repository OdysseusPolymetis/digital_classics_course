{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OdysseusPolymetis/digital_classics_course/blob/main/8_bases_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WytjGo_gY_92"
      },
      "source": [
        "# **Les Transformers, quelques exemples de base**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ8P_wKTY_92"
      },
      "source": [
        "Les transformers permettent d'encapsuler les mots dans leur contexte. On les appelle des transformers parce qu'un seul modèle peut servir pour faire plein de choses (détection d'entités nommées, analyse de sentiment, génération de texte, etc.). Voilà comment on les fait marcher, simplement."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1sGd6mhCoT3eEYgwz776OmckPrvsEcbdB' width=\"1000\">"
      ],
      "metadata": {
        "id": "0yDAHK1CYTzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1hjziU7wTdSiySpMFuFIf4xi1Dxn5Hgbt' width=\"1000\">"
      ],
      "metadata": {
        "id": "7VIEBVdiYlcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici donc quelques exemples."
      ],
      "metadata": {
        "id": "b1DIQmzYZBtd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le premier exemple part d'un XLM Roberta multilingue (il couvre près de 100 langues, dont le latin)."
      ],
      "metadata": {
        "id": "OUjgjGsYqvpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "clf = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"joeddav/xlm-roberta-large-xnli\"\n",
        ")\n",
        "\n",
        "text = \"Traité de paix entre le Roi, le roi d'Espagne et le roi de la Grande Bretagne, avec l'accession du roi de Portugal\tFrance. Secrétariat d'Etat aux affaires étrangères (1589-1791). Auteur du texte\timp. royale (Paris)\t1763\"\n",
        "labels = [\"justice\", \"computers\", \"ebook\"]\n",
        "res = clf(text, candidate_labels=labels, hypothesis_template=\"This text is {}.\")\n",
        "print(res)"
      ],
      "metadata": {
        "id": "i5SvE6HJijEU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aaa4e0c-28a6-48a0-b125-1c4a4e4733b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sequence': \"Traité de paix entre le Roi, le roi d'Espagne et le roi de la Grande Bretagne, avec l'accession du roi de Portugal\\tFrance. Secrétariat d'Etat aux affaires étrangères (1589-1791). Auteur du texte\\timp. royale (Paris)\\t1763\", 'labels': ['justice', 'computers', 'ebook'], 'scores': [0.48937487602233887, 0.3095439672470093, 0.20108120143413544]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyZWiCJbY_93"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "text = \"Odi et amo; quare id faciam, fortasse requiris.\"\n",
        "labels = [\"positive\", \"negative\", \"neutral\"]\n",
        "res = clf(text, candidate_labels=labels, hypothesis_template=\"This text is {}.\")\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "lZifqhIt0WBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "\trepo_id=\"tensorblock/hathibelagal_llama-3.2-latin-GGUF\",\n",
        "\tfilename=\"llama-3.2-latin-Q5_K_M.gguf\",\n",
        "  n_ctx=1024,\n",
        "  n_gpu_layers=-1,\n",
        "  verbose=False\n",
        ")\n",
        "output = llm(\n",
        "\t\"Odi et amo; quare id faciam, fortasse requiris.\",\n",
        "\tmax_tokens=120,\n",
        "  repeat_penalty=1.1\n",
        ")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "oJYzk2sZ0Tgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpLwlgzdY_94"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\")\n",
        "unmasker(\"Magna spes in <mask> est.\", top_k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMttBhBOY_94"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner = pipeline(\"ner\", grouped_entities=True)\n",
        "ner(\"Caesar in Galliam profectus est.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sKE0wweY_95"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "question_answerer(\n",
        "    question=\"Where do I work?\",\n",
        "    context=\"My name is Marianne and I usually do my stuff at the ENS de Lyon.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr9fNUYKY_96"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer(\n",
        "    \"\"\"\n",
        "    America has changed dramatically during recent years. Not only has the number of\n",
        "    graduates in traditional engineering disciplines such as mechanical, civil,\n",
        "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
        "    the premier American universities engineering curricula now concentrate on\n",
        "    and encourage largely the study of engineering science. As a result, there\n",
        "    are declining offerings in engineering subjects dealing with infrastructure,\n",
        "    the environment, and related issues, and greater concentration on high\n",
        "    technology subjects, largely supporting increasingly complex scientific\n",
        "    developments. While the latter is important, it should not be at the expense\n",
        "    of more traditional engineering.\n",
        "\n",
        "    Rapidly developing economies such as China and India, as well as other\n",
        "    industrial countries in Europe and Asia, continue to encourage and advance\n",
        "    the teaching of engineering. Both China and India, respectively, graduate\n",
        "    six and eight times as many traditional engineers as does the United States.\n",
        "    Other industrial countries at minimum maintain their output, while America\n",
        "    suffers an increasingly serious decline in the number of engineering graduates\n",
        "    and a lack of well-educated engineers.\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbaRONEeY_96"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
        "translator(\"Ce cours est fait pour EnExDi et les Humanités Numériques.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}