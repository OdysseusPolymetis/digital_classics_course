{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OdysseusPolymetis/digital_classics_course/blob/main/2_nlp_greek_latin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z74vm47glyC"
      },
      "source": [
        "#**ANALYSE TEXTUELLE : tokénisation, lemmatisation, syntaxe et entités nommées**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SREcs0t-OqAz"
      },
      "outputs": [],
      "source": [
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt0eU-qsLb10"
      },
      "source": [
        "##Qu'est-ce que c'est que l'analyse textuelle (automatique), rapidement\n",
        "En fait ça peut toucher beaucoup de domaines. Ici, on va en voir plusieurs, entre autres la **tokenisation**, la **lemmatisation**, le **postagging**. J'appellerai ça les étapes de \"pre-processing\", de travail préliminaire.\n",
        "<br>En effet, on a très souvent besoin, pour faire des choses plus poussées, de ces étapes pour éviter de créer des biais dans les analyses qui vont suivre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLIKh_80WtdM"
      },
      "source": [
        "##Comment on pré-traite ?\n",
        "Il y a plusieurs écoles, et donc aussi plusieurs modules pour le faire.\n",
        "<br>D'expérience personnelle, il y a les modules rapides et qui donnent des résultats plus mitigés, et des modules plus longs qui sont généralement meilleurs (mais pas forcément).\n",
        "<br>Ici, je vais vous montrer plusieurs outils avec des qualités et des défauts, pour le français (mais ils ont aussi des modèles en d'autres langues)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31i7LBEP1HF9"
      },
      "source": [
        "##**Quelques outils en ligne utiles**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P_eXTzr1Uup"
      },
      "source": [
        "###**UDPipe**\n",
        "<br>Vous le trouverez [ici](https://lindat.mff.cuni.cz/services/udpipe/).\n",
        "<br>C'est un bon outil pour un texte court. Dès que vous allez faire un texte intégral ou un peu long, ça rame très vite.\n",
        "<br>Mais vous avez des fonctionnalités sympathiques, type les arbres syntaxiques en images vectorielles.**texte en gras**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDSIHDth4kxu"
      },
      "source": [
        "###**Deucalion**\n",
        "<br>Vous le trouverez [ici](https://dh.chartes.psl.eu/deucalion/api/fr/).\n",
        "<br>Bien plus gérable pour les textes longs. Une sortie pas sexy, mais ce n'est pas ce qu'on lui demande. Il est surtout très efficace pour des langues plus rares, type l'ancien français ou le grec et le latin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbFh1tm45B9B"
      },
      "source": [
        "###**VoyantTools**\n",
        "<br>Vous le trouverez [ici](https://voyant-tools.org/).\n",
        "<br>C'est un outil de visualisation. C'est pratique et un peu shiny, mais on peut faire des choses très développées dessus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj7F-HDpcCkK"
      },
      "source": [
        "#**LE TAL : TOKENISATION, LEMMATISATION, POSTAGGING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQIzGqJzcUVk"
      },
      "source": [
        "Nous allons tester un outil principal, qui permet de faire ces trois actions, **`stanza`**. Il en existe beaucoup d'autres (notamment spacy et pie-extended), mais stanza est le plus performant en termes de rapport qualité/temps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR5RRUfQc5u4"
      },
      "outputs": [],
      "source": [
        "catilinaires=\"Quousque tandem abutere, Catilina, patientia nostra ? Quamdiu etiam furor iste tuus nos eludet ? Quem ad finem sese effrenata jactabit audacia ? Nihilne te nocturnum praesidium Palatii, nihil urbis vigiliae, nihil timor populi, nihil concursus bonorum omnium, nihil hic munitissimus habendi senatus locus, nihil horum ora vultusque moverunt ? Patere tua consilia non sentis ? Constrictam jam horum omnium scientia teneri conjurationem tuam non vides ? Quid proxima, quid superiore nocte egeris, ubi fueris, quos convocaveris, quid consilii ceperis, quem nostrum ignorare arbitraris ? O tempora ! O mores ! Senatus haec intellegit, consul videt. Hic tamen vivit.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r_DV1BTw4Cr"
      },
      "source": [
        "##**stanza (précédemment Stanford CoreNLP)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCtmOg83d83p"
      },
      "source": [
        "Personnellement, c'est l'analyseur que j'utilise quand je n'ai pas besoin de faire de représentations graphiques de mes résultats. Il est rapide et efficace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN7vhzYuWTO7"
      },
      "source": [
        "Je mentionne `stanza` en particulier pour trois raisons :\n",
        "<br>- d'abord parce qu'il dispose d'un très grand nombre de modèles de langue, et pas forcément des langues très répandues,\n",
        "<br>- ensuite parce qu'il est très rapide, et niveau performance tout à fait satisfaisant pour les gros corpus,\n",
        "<br>- enfin parce que je le trouve facile à manipuler et à implémenter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mREr1vy_WzEY"
      },
      "source": [
        "Mais il faut garder en tête qu'il en existe bien d'autres qui fonctionnent vraiment très bien, avec un nombre de modèles qui se multiplie. Je pense au tagger de BERT,  ou de `flair` entre autres. Mais ça nécessite d'être un peu plus aguerri."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPEkXPIxeUob"
      },
      "source": [
        "Là encore, il existe plusieurs modèles rien que pour le français (je vous mets ici la [liste des modèles](https://stanfordnlp.github.io/stanza/available_models.html) dans d'autres langues), mais le modèle par défaut peut être appelé avec `la` pour le latin et `grc` pour le grec ancien."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJXi9pljw2DR"
      },
      "outputs": [],
      "source": [
        "import stanza\n",
        "stanza.download('la', package=\"perseus\")\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKCT-NaPxpbp"
      },
      "source": [
        "On commence par lui spécifier une Pipeline, c'est-à-dire qu'on lui signifie quels processeurs il va devoir mobiliser pour les opérations suivantes et en quelle langue. L'avantage de cette opération est qu'on ne mobilise pas l'artillerie lourde quand on veut faire des opérations simples)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x5kKE7rxTgB"
      },
      "outputs": [],
      "source": [
        "nlp_stanza = stanza.Pipeline(lang='la', package=\"perseus\", processors='tokenize,pos,lemma, depparse')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUd62DkjgcvW"
      },
      "source": [
        "Maintenant, nous pouvons lancer le TAL sur l'ensemble du texte (il est beaucoup plus rapide que `spacy` à cet égard)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMPY243ZyE8S"
      },
      "outputs": [],
      "source": [
        "catilinaires_analyzed=nlp_stanza(catilinaires)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSS-MkX_gr7m"
      },
      "source": [
        "Voyons maintenant ses résultats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQPdO0otBqA8"
      },
      "outputs": [],
      "source": [
        "for sent in catilinaires_analyzed.sentences:\n",
        "  print(\"XXXXX \"+sent.text+\" XXXXX\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNXzUVFZzojD"
      },
      "outputs": [],
      "source": [
        "for sent in catilinaires_analyzed.sentences:\n",
        "  for token in sent.words:\n",
        "    print(token.text + ' - ' + token.lemma + ' - ' + token.pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avec votre texte maintenant"
      ],
      "metadata": {
        "id": "etySMAjIWQZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalement, vous avez sur vous une sortie d'HTR presque propre.\n",
        "<br>Vous aurez besoin d'un texte enregistré en \".txt\". Je ne garantis pas la qualité des résultats.\n",
        "<br>Une fois que vous avez votre texte, vous devez appuyer sur l'icône \"dossier\" sur la gauche de l'écran, et faire glisser le fichier dans la colonne blanche jusqu'à voir un \"+\". Votre texte va s'importer dans l'environnement Colab.\n",
        "<br>Nous allons maintenant lancer une analyse automatique avec `stanza`, qui sera en latin par défaut ici.\n",
        "<br>Nous ne pourrons pas le faire exactement de la même manière que précédemment car le texte est plus long, et il faudra ménager un peu la mémoire de colab.\n",
        "<br>Si vous travaillez sur du grec, changez simplement les endroits où vous voyez `\"la\"` en `\"grc\"`."
      ],
      "metadata": {
        "id": "oIHZZa6nWU9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "J'utilise généralement `stanza` pour trois raisons :\n",
        "<br>- il y a un très grand nombre de langues traitées (Vous pouvez les consulter [ici](https://stanfordnlp.github.io/stanza/performance.html)),\n",
        "<br>- c'est très rapide et ça fait un excellent usage de la GPU,\n",
        "<br>- c'est facile à implémenter."
      ],
      "metadata": {
        "id": "CzcrFiiZZCx1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHGyvw9k7HD3"
      },
      "outputs": [],
      "source": [
        "def batch_process(text, nlp, batch_size=50):\n",
        "    paragraphs = text.split('\\n')\n",
        "    batches = [paragraphs[i:i + batch_size] for i in range(0, len(paragraphs), batch_size)]\n",
        "\n",
        "    words = []\n",
        "\n",
        "    for batch in batches:\n",
        "        batch_text = '\\n'.join(batch)\n",
        "        doc = nlp(batch_text)\n",
        "        for sentence in doc.sentences:\n",
        "            for word in sentence.words:\n",
        "                token={}\n",
        "                if word.lemma is not None:\n",
        "                    token[\"word\"]=word.text\n",
        "                    token[\"lemma\"]=word.lemma\n",
        "                    token[\"pos\"]=word.pos\n",
        "                    words.append(token)\n",
        "\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "import string"
      ],
      "metadata": {
        "id": "f2bHYAFOYfpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici je vous propose un texte par défaut, l'_Odyssée_ intégrale, mais vous pouvez tout aussi bien:\n",
        "\n",
        "\n",
        "*   soit ouvrir le fichier `odyssee_integrale.txt` que vous verrez si vous appuyez sur l'icône dossier sur la gauche (vous double-cliquez, vous collez votre texte et vous faites un ctrl+S pour sauvegarder),\n",
        "*   soit faire un \"drag and drop\" de votre fichier en txt lorsque vous aurez appuyé sur l'icône dossier sur la gauche (veillez à bien droper sur le blanc, et pas sur le dossier). Vous devrez ensuite, dans la cellule `filepath_of_text = \"/content/odyssee_integrale.txt\"` changer le nom en mettant bien le nom de votre fichier, et en conservant \"/content/\".\n",
        "\n"
      ],
      "metadata": {
        "id": "mOJhswUuizbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/OdysseusPolymetis/digital_classics_course/refs/heads/main/odyssee_integrale.txt"
      ],
      "metadata": {
        "id": "zgUE5AS2NVSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath_of_text = \"/content/odyssee_integrale.txt\""
      ],
      "metadata": {
        "id": "zCCJtvF2ZP1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text = open(filepath_of_text, encoding=\"utf-8\").read()"
      ],
      "metadata": {
        "id": "Jb1ll9BDZS6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beaucoup de modèles ont beaucoup de processus embarqués, et sont trop lourds. Je vous recommande d'être plus sélectifs lors de l'instanciation des processus. Vous pouvez le faire de cette manière :"
      ],
      "metadata": {
        "id": "gRtrtUH7ZIOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_stanza = stanza.Pipeline(lang='grc', processors='tokenize,lemma')"
      ],
      "metadata": {
        "id": "bAPzFT46YugW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette cellule, selon le texte que vous avez mis, peut être longue. C'est la cellule de lemmatisation."
      ],
      "metadata": {
        "id": "CYVNYGcYj0td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyzed_text = batch_process(full_text, nlp_stanza)"
      ],
      "metadata": {
        "id": "xIBpz_4nYjkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "là c'est un aperçu de ce qu'il a obtenu en lemmatisant."
      ],
      "metadata": {
        "id": "Ggb63YWBj70c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(analyzed_text[5:15])"
      ],
      "metadata": {
        "id": "x0tlOmanZaAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour une de nos expériences à venir, nous allons avoir besoin d'une liste de mots outils, que vous pourrez modifier à votre guise (il s'agit, pour le latin, du fichier stopwords_lat.txt, et pour le grec, du fichier stopwords_gk.txt).\n",
        "<br>Si vous souhaitez utiliser le fichier pour le latin, faites ce code :\n",
        "```python\n",
        "!wget https://raw.githubusercontent.com/OdysseusPolymetis/digital_classics_course/refs/heads/main/stopwords_lat.txt\n",
        "```\n",
        "et pour le grec :\n",
        "```python\n",
        "!wget https://raw.githubusercontent.com/OdysseusPolymetis/digital_classics_course/refs/heads/main/stopwords_gk.txt\n",
        "```\n"
      ],
      "metadata": {
        "id": "QRR7NZ8naKEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/OdysseusPolymetis/digital_classics_course/refs/heads/main/stopwords_gk.txt"
      ],
      "metadata": {
        "id": "93Do-k0Ja05o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = open(\"/content/stopwords_gk.txt\",'r',encoding=\"utf8\").read().split(\"\\n\")"
      ],
      "metadata": {
        "id": "-M_2igj5a2KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette cellule là l'aide à ne pas trop tenir compte des accentuations (en grec ça peut être un vrai problème, parce qu'un accent qui apparaîtra de la même manière, peut être encodé de plusieurs manières différentes)."
      ],
      "metadata": {
        "id": "XhvkPRmPkD__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "def strip_diacritics(s: str) -> str:\n",
        "    decomposed = unicodedata.normalize(\"NFD\", s)\n",
        "    without_marks = ''.join(ch for ch in decomposed\n",
        "                            if unicodedata.category(ch) != \"Mn\")\n",
        "    return without_marks.casefold()"
      ],
      "metadata": {
        "id": "qyW4-_9DhjaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_stopwords = {strip_diacritics(w) for w in stopwords}"
      ],
      "metadata": {
        "id": "tGSGxGCbhrxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici nous faisons trois listes, une qui va contenir le texte, une les lemmes, et une les lemmes sans la ponctuation et sans les mots outils."
      ],
      "metadata": {
        "id": "K3Fx9pnakQCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forms = []\n",
        "lemmas = []\n",
        "no_stop = []\n",
        "\n",
        "for token in analyzed_text:\n",
        "    form = token[\"word\"]\n",
        "    lemma = token[\"lemma\"]\n",
        "\n",
        "    if lemma not in string.punctuation:\n",
        "        forms.append(form)\n",
        "        lemmas.append(lemma)\n",
        "\n",
        "    lemma_norm = strip_diacritics(lemma)\n",
        "\n",
        "    if (\n",
        "        lemma not in string.punctuation and\n",
        "        lemma_norm not in normalized_stopwords\n",
        "    ):\n",
        "\n",
        "        no_stop.append(lemma)"
      ],
      "metadata": {
        "id": "KQMQx7NTa_Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "là on va vérifier la longueur des listes."
      ],
      "metadata": {
        "id": "5E2IZo-VkYEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(lemmas)"
      ],
      "metadata": {
        "id": "nRAm6ZDsdHBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(no_stop)"
      ],
      "metadata": {
        "id": "qGTu3w4hdI4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si vous faites du grec, par défaut, certaines polices ne marchent pas bien. La cellule qui suit télécharge une police sur github, qui marche généralement un peu avec toutes les langues."
      ],
      "metadata": {
        "id": "e0-mfwlodOws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/helvetica-font/helvetica-font.github.io/raw/refs/heads/master/fonts/Helvetica.ttf"
      ],
      "metadata": {
        "id": "ndQbiwmydbSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "DEFAULT_FONT = \"/content/Helvetica.ttf\"\n",
        "\n",
        "def create_word_cloud(words_list, title):\n",
        "    text = ' '.join(words_list)\n",
        "\n",
        "    radius = 495\n",
        "    diameter = radius * 2\n",
        "    center = radius\n",
        "    x, y = np.ogrid[:diameter, :diameter]\n",
        "    mask = (x - center) ** 2 + (y - center) ** 2 > radius ** 2\n",
        "    mask = 255 * mask.astype(int)\n",
        "\n",
        "    mask_rgba = np.dstack((mask, mask, mask, 255 - mask))\n",
        "\n",
        "    wordcloud = WordCloud(\n",
        "        repeat=False,\n",
        "        width=diameter,\n",
        "        height=diameter,\n",
        "        background_color=None,\n",
        "        mode=\"RGBA\",\n",
        "        colormap='plasma',\n",
        "        mask=mask_rgba,\n",
        "        font_path=DEFAULT_FONT,\n",
        "    ).generate(text)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9qv1DH7LdMhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_word_cloud(forms, 'Word Cloud for Forms')"
      ],
      "metadata": {
        "id": "h-wt9rK1eJpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_word_cloud(lemmas, 'Word Cloud for Lemmas')"
      ],
      "metadata": {
        "id": "t5B5uCAEeL5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_word_cloud(no_stop, 'Word Cloud for Lemmas without stopwords')"
      ],
      "metadata": {
        "id": "pKzLFQ6hec7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Par ailleurs, il m'a semblé utile de joindre les trois listes sous forme de fichiers téléchargeables, afin que vous puissiez vous en servir pour d'autres expériences. C'est la cellule qui suit, qui vous fait un zip avec les trois listes. Si vous ne voulez pas télécharger, inutile de faire la suite."
      ],
      "metadata": {
        "id": "b6uYeVUUkeCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "export_dir = \"/content/export_lists\"\n",
        "os.makedirs(export_dir, exist_ok=True)\n",
        "\n",
        "def write_list_to_txt(filename, items):\n",
        "    path = os.path.join(export_dir, filename)\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in items:\n",
        "            f.write(str(item) + \"\\n\")\n",
        "    return path\n",
        "\n",
        "forms_path   = write_list_to_txt(\"forms.txt\", forms)\n",
        "lemmas_path  = write_list_to_txt(\"lemmas.txt\", lemmas)\n",
        "no_stop_path = write_list_to_txt(\"no_stop.txt\", no_stop)"
      ],
      "metadata": {
        "id": "7WRBYb9fegnJ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "zip_base = \"/content/text_lists\"\n",
        "shutil.make_archive(zip_base, \"zip\", export_dir)\n",
        "\n",
        "zip_path = zip_base + \".zip\"\n",
        "print(\"Archive créée :\", zip_path)\n",
        "\n",
        "files.download(zip_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "R8s5gjxJlaFK",
        "outputId": "d8b1c550-1880-46fa-e795-54a181b11cc7"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive créée : /content/text_lists.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_85b89529-3f60-407a-be14-b63ac355a81c\", \"text_lists.zip\", 685814)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b4x-KjT9lkk0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1hxMNZZBdsT9AyRKnXcdOnis82XnHDehl",
      "authorship_tag": "ABX9TyNirpGhvZCMHf2R1oiWBNza",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}