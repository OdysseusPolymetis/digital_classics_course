{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+usW6F3GwjgRV+vAcLzYN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OdysseusPolymetis/digital_classics_course/blob/main/7_word_vectors_lat_gk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Vecteurs de mots statiques pour le grec ancien et le latin**\n",
        "---\n"
      ],
      "metadata": {
        "id": "F44-RzJtRAUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans ce notebook, vous pourrez prendre le texte de votre choix (grec ou latin, mais limité) ici : [Perseus Treebank](https://github.com/PerseusDL/treebank_data.git)."
      ],
      "metadata": {
        "id": "Bwo5xJSBRdnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons utiliser cette banque parce que la plupart des données a été à peu près contrôlée, et correctement lemmatisée. Ça nous évitera le traitement par des modules tiers."
      ],
      "metadata": {
        "id": "S-mDSxm0RzEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Préparation des phrases\n"
      ],
      "metadata": {
        "id": "FYrqm1RpbkL1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i91dUjI1Q0vV"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/PerseusDL/treebank_data.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "J'ajoute une liste de mots outils : décommentez la commande dont vous avez besoin en fonction de votre choix de langue."
      ],
      "metadata": {
        "id": "c0tM6TShVXfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/OdysseusPolymetis/digital_classics_course/main/stopwords_gk.txt\n",
        "#!wget https://raw.githubusercontent.com/OdysseusPolymetis/digital_classics_course/main/stopwords_lat.txt"
      ],
      "metadata": {
        "id": "SMk-5BXFShiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import glob"
      ],
      "metadata": {
        "id": "Lnw6KdksViQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deux fonctions à suivre, une qui charge les mots outils (attention au chemin de fichier quand vous l'appellerez), et l'autre qui parcourt les dossiers (même remarque)."
      ],
      "metadata": {
        "id": "Y1VACp6WaBWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_stopwords(path_txt):\n",
        "    with open(path_txt, \"r\", encoding=\"utf-8\") as f:\n",
        "        return {line.strip() for line in f if line.strip() and not line.lstrip().startswith(\"#\")}"
      ],
      "metadata": {
        "id": "U8dFFRG4aMfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans cette fonction, vous avez l'option `stopwords`, que vous pourrez activer ou non à l'appel (par défaut ce sera activé)."
      ],
      "metadata": {
        "id": "pGvBKUC9bBPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmas_by_sentence(xml_path, include_punct=False, stopwords=None):\n",
        "    xml_path = Path(xml_path)\n",
        "    sentences = []\n",
        "    current = []\n",
        "\n",
        "    for event, elem in ET.iterparse(xml_path, events=(\"start\", \"end\")):\n",
        "        tag = elem.tag\n",
        "\n",
        "        if event == \"start\" and tag == \"sentence\":\n",
        "            current = []\n",
        "\n",
        "        elif event == \"end\" and tag == \"word\":\n",
        "            postag = elem.attrib.get(\"postag\", \"\")\n",
        "            if not include_punct and postag.startswith(\"u\"):\n",
        "                pass\n",
        "            else:\n",
        "                lemma = elem.attrib.get(\"lemma\")\n",
        "                if lemma:\n",
        "                    if not stopwords or lemma not in stopwords:\n",
        "                        current.append(lemma)\n",
        "\n",
        "        elif event == \"end\" and tag == \"sentence\":\n",
        "            sentences.append(current)\n",
        "            elem.clear()\n",
        "\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "tbusR3r1VoPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour la première variable `text_folder`, vous devez mettre le chemin exact qui mène au dossier auteur (si vous voulez traiter un auteur en entier) ou au dossier texte. La seconde variable `auteur` devra contenir le code auteur de votre choix (code urn sur phi ou tlg, exemple : pour Homère, tlg0012)."
      ],
      "metadata": {
        "id": "4wGpv0zTWVUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_folder = \"/content/treebank_data/v2.1/Greek/texts\"\n",
        "auteur = \"tlg0012\""
      ],
      "metadata": {
        "id": "PLjEpu-9V5h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici, selon que vous utilisez le grec ou le latin, décommentez la ligne dont vous avez besoin, commentez l'autre."
      ],
      "metadata": {
        "id": "ZnJsBooHaeLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = load_stopwords(\"/content/stopwords_gk.txt\")\n",
        "#stopwords = load_stopwords(\"/content/stopwords_lat.txt\")"
      ],
      "metadata": {
        "id": "GpRaxNSOaYpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = sorted(glob.glob(os.path.join(text_folder, \"**\", f\"{auteur}.*.xml\"), recursive=True))"
      ],
      "metadata": {
        "id": "_89gIvmQXw0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [sentence for f in files for sentence in lemmas_by_sentence(f, stopwords=stopwords)]"
      ],
      "metadata": {
        "id": "Cy_t733cZDWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On vérifie simplement que les phrases sont bien prises en charge :"
      ],
      "metadata": {
        "id": "B3S5cA2wZV01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZYLFWS4ZE_9",
        "outputId": "657c3044-9d59-4320-b123-96bc8401cdc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15138"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Vectorisation"
      ],
      "metadata": {
        "id": "YhcwYrLbbx69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour effectuer la vectorisation, nous allons passer par une librairie classique, gensim (dispo [ici](https://radimrehurek.com/gensim/))."
      ],
      "metadata": {
        "id": "u99UUlLHcHKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "jpytAajfcA9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "Oj5E_ymUZalW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La cellule qui suit va permettre de constituer le modèle. Le temps que ça prendra dépendra des paramètres que vous mettrez. Voilà une brève explication.\n",
        "\n",
        "\n",
        "*   `sentences` : c'est la liste de phrases lemmatisées\n",
        "*   `min_count` : c'est le nombre minimal de fois qu'un mot doit apparaître pour être pris en compte\n",
        "*   `max_vocab_size` : c'est le nombre de mots max qui va être compris dans le modèle\n",
        "*   `negative` : le modèle voit des paires vraies (mot en contexte) et, pour chaque paire vraie, il voit aussi des paires fausses (ce qu'on appelle adversarial training) choisies au hasard, ici X faux voisins par vrai voisin : théoriquement, plus cette valeur est haute, plus l’apprentissage est précis, et coûteux\n",
        "*   `epochs` : nombre de fois où le modèle parcourt tout le corpus\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ccrno3H-cm6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences, min_count=2, max_vocab_size=10000, negative=50, epochs=300)"
      ],
      "metadata": {
        "id": "Sl58Ui1Sb7lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(positive=[\"γυνή\",\"θεός\"], negative=[\"ἀνήρ\"],topn=10)"
      ],
      "metadata": {
        "id": "NJT31LDBcmPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('θεός',topn=20)"
      ],
      "metadata": {
        "id": "-Z87y2zrfCp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Et voici un petit bout de code pour exporter vos données et les visualiser plus aisément. Vous pouvez vous rendre sur le site de projection de [tensorflow](https://projector.tensorflow.org/), appuyer sur le bouton `load` sur la gauche, mettre le fichier de `1_vecteurs.tsv` en première option du pop-up, et le fichier `2_metadonnees.tsv` dans la seconde."
      ],
      "metadata": {
        "id": "RHvAis3FhjKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/1_vecteurs.tsv\", 'w') as file_vectors, open(\"/content/2_metadonnees.tsv\", 'w') as file_metadata:\n",
        "    for word in model.wv.index_to_key:\n",
        "        file_vectors.write('\\t'.join([str(x) for x in model.wv[word]]) + \"\\n\")\n",
        "        file_metadata.write(word + \"\\n\")"
      ],
      "metadata": {
        "id": "Ry221Oc2hLpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Avec votre propre texte**"
      ],
      "metadata": {
        "id": "FI1stx7O5aqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici même principe, mais avec un txt. Plus le txt est long, mieux ça marchera."
      ],
      "metadata": {
        "id": "PHtTfuR05frX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza"
      ],
      "metadata": {
        "id": "9hsImc2kik4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans la cellule suivante, veillez à changer `\"/content/stopwords_lat.txt\"` en `\"/content/stopwords_grc.txt\"` en fonction de la langue que vous aurez choisie"
      ],
      "metadata": {
        "id": "1PYJ-T9I5vTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = open(\"/content/stopwords_lat.txt\",'r',encoding=\"utf8\").read().split(\"\\n\")"
      ],
      "metadata": {
        "id": "2HT58tHm5sDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans la cellule suivante, vous allez pouvoir choisir un fichier à traiter."
      ],
      "metadata": {
        "id": "ODQFK2_Z6fY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "OhQ20IEW6jFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici vous devez mettre le code langue pour l'analyse."
      ],
      "metadata": {
        "id": "ang9Mf2i6yja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LANG = \"la\""
      ],
      "metadata": {
        "id": "IryJLvki6x6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device utilisé :\", DEVICE, \"| Langue :\", LANG)"
      ],
      "metadata": {
        "id": "DuSOfStc7BnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stanza.download(LANG)\n",
        "nlp = stanza.Pipeline(\n",
        "    lang=LANG,\n",
        "    processors=\"tokenize,lemma\",\n",
        "    use_gpu=(DEVICE == \"cuda\"),\n",
        "    device=DEVICE\n",
        ")"
      ],
      "metadata": {
        "id": "srKrajrB7AYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans cette cellule, vous veillerez à mettre le bon nom de fichier (votre fichier). Attention, pas d'espaces ou de caractères spéciaux dans le titre."
      ],
      "metadata": {
        "id": "ZLO1DIN16kXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath_of_text = \"/content/votreTexte.txt\""
      ],
      "metadata": {
        "id": "l4oP4o2J7ZtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text = open(filepath_of_text, encoding=\"utf-8\").read()"
      ],
      "metadata": {
        "id": "moV2khB67duk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_process_to_lemmas(text, nlp, batch_size=100):\n",
        "    paragraphs = text.split('\\n')\n",
        "    batches = [paragraphs[i:i + batch_size] for i in range(0, len(paragraphs), batch_size)]\n",
        "\n",
        "    sentences_lemmas = []\n",
        "\n",
        "    for batch in batches:\n",
        "        batch_text = '\\n'.join(batch)\n",
        "        doc = nlp(batch_text)\n",
        "        for sentence in doc.sentences:\n",
        "            sentence_lemmas = []\n",
        "            for word in sentence.words:\n",
        "                if word.lemma is not None and word.lemma not in stopwords:\n",
        "                    sentence_lemmas.append(word.lemma.lower())\n",
        "            sentences_lemmas.append(sentence_lemmas)\n",
        "\n",
        "    return sentences_lemmas"
      ],
      "metadata": {
        "id": "bYx8AG3w7hzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = batch_process_to_lemmas(full_text, nlp_stanza)"
      ],
      "metadata": {
        "id": "2xEFFONb7kX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette cellule-là vous permet d'entraîner le modèle. Pensez à regarder les différents paramètres."
      ],
      "metadata": {
        "id": "t-oEqPCq85b0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences, min_count=2, max_vocab_size=10000, negative=10, epochs=300)"
      ],
      "metadata": {
        "id": "ZWqVW4617nNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici petite cellule de test pour voir si tout fonctionne."
      ],
      "metadata": {
        "id": "2lJ9pnuL9DAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('amicitia',topn=50)"
      ],
      "metadata": {
        "id": "Ep8QUxdB7pjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici c'est l'export de vos fichiers."
      ],
      "metadata": {
        "id": "Bmv6IqTb9G69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "export_dir = \"/content/output\"\n",
        "os.makedirs(export_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "dm95AtqH9MfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/output/vecteurs.tsv\", 'w') as file_vectors, open(\"/content/output/metadonnees.tsv\", 'w') as file_metadata:\n",
        "    for word in model.wv.index_to_key:\n",
        "        file_vectors.write('\\t'.join([str(x) for x in model.wv[word]]) + \"\\n\")\n",
        "        file_metadata.write(word + \"\\n\")"
      ],
      "metadata": {
        "id": "M9Nkw_GY7rTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette cellule vous permet de télécharger vos résultats pour les mettre ensuite dans le [tensorflow projector](https://projector.tensorflow.org/)."
      ],
      "metadata": {
        "id": "ZBzDcpmf8oJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "zip_base = \"/content/output\"\n",
        "shutil.make_archive(zip_base, \"zip\", export_dir)\n",
        "\n",
        "zip_path = zip_base + \".zip\"\n",
        "print(\"Archive créée :\", zip_path)\n",
        "\n",
        "files.download(zip_path)"
      ],
      "metadata": {
        "id": "SQMtGDs_7_su"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}