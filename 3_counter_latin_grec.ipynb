{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OdysseusPolymetis/digital_classics_course/blob/main/3_counter_latin_grec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Quelques représentations statistiques de base**"
      ],
      "metadata": {
        "id": "Ur72hDxRnHqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici quelques représentations graphiques auxquelles on peut réfléchir pour comparer lexicalement plusieurs auteurs."
      ],
      "metadata": {
        "id": "nmWqsd2r7cTU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjI3A9LOJF32"
      },
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncZnRPUhJjn2"
      },
      "outputs": [],
      "source": [
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KprXRWI_JTXV"
      },
      "outputs": [],
      "source": [
        "import stanza\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSaQUHE4x5gd"
      },
      "source": [
        "## **Quelques statistiques de base**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiPSAZcoxutr"
      },
      "source": [
        "Sur vos propres textes, quelques visualisations statistiques.\n",
        "<br>Vous devez, dans un premier temps, vous resservir du premier notebook que nous avons fait (imports cltk), et télécharger le zip final qui contiendra vos données. Une fois ce zip obtenu, dézippez sur votre ordinateur, et parcourez-le jusqu'à tomber sur le dossier des données qui vous intéressent. Cela fait, compressez le en le nommant corpus.zip.\n",
        "<br>Dans la cellule suivante, importer le zip que vous venez de créer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "dEsmVWLcXTzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La cellule qui suit permet de dézipper. À l'issue de cette cellule, vous devez obtenir un dossier avec vos données. Certaines seront en xml, d'autres en txt. Je n'aurai peut-être pas tout testé et vérifié."
      ],
      "metadata": {
        "id": "cfVm0OBFZlIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/corpus.zip -d /content/"
      ],
      "metadata": {
        "id": "7IfXzupSZzu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from lxml import etree\n",
        "\n",
        "base_dir = \"/content/corpus\"\n",
        "texts = []"
      ],
      "metadata": {
        "id": "Fi9SzgBsZ5w4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_body_text_from_xml(path):\n",
        "\n",
        "    parser = etree.XMLParser(recover=True, resolve_entities=False)\n",
        "\n",
        "    with open(path, \"rb\") as f:\n",
        "        tree = etree.parse(f, parser)\n",
        "\n",
        "    root = tree.getroot()\n",
        "\n",
        "    body = root.find('.//{*}body')\n",
        "    if body is None:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "    text = \" \".join(body.itertext())\n",
        "\n",
        "    text = \" \".join(text.split())\n",
        "    return text"
      ],
      "metadata": {
        "id": "YeyBl9VMaKwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fast_vocab_stats(file_path, text, nlp, max_chars=200_000):\n",
        "    print(\"Nom du fichier traité : \"+file_path)\n",
        "\n",
        "    paragraphs = text.split('\\n')\n",
        "    chunks = []\n",
        "    current = []\n",
        "    current_len = 0\n",
        "\n",
        "    for p in paragraphs:\n",
        "        length = len(p) + 1\n",
        "        if current and current_len + length > max_chars:\n",
        "            chunks.append('\\n'.join(current))\n",
        "            current = [p]\n",
        "            current_len = length\n",
        "        else:\n",
        "            current.append(p)\n",
        "            current_len += length\n",
        "    if current:\n",
        "        chunks.append('\\n'.join(current))\n",
        "\n",
        "    total = 0\n",
        "    lemmas_set = set()\n",
        "\n",
        "    for chunk in chunks:\n",
        "        if not chunk.strip():\n",
        "            continue\n",
        "        doc = nlp(chunk)\n",
        "        for sentence in doc.sentences:\n",
        "            for word in sentence.words:\n",
        "                if word.lemma:\n",
        "                    total += 1\n",
        "                    lemmas_set.add(word.lemma)\n",
        "\n",
        "    return total, len(lemmas_set)"
      ],
      "metadata": {
        "id": "BX6vSLf-cqZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici, en fonction de ce que vous aurez choisi (du grec ou du latin), choisissez `grc` ou `la` dans la cellule ci-dessous."
      ],
      "metadata": {
        "id": "eip___XRiJGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_stanza = stanza.Pipeline(\n",
        "    'la',\n",
        "    processors='tokenize,pos,lemma',\n",
        "    use_gpu=True,\n",
        "    tokenize_batch_size=5000,\n",
        "    pos_batch_size=2000,\n",
        "    lemma_batch_size=2000,\n",
        "    verbose=False,\n",
        ")"
      ],
      "metadata": {
        "id": "iGTM1HcReFc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette partie-là peut mettre pas mal de temps, soyez patients (ça va dépendre de l'auteur que vous aurez soumis).\n",
        "<br>Pour tout Cicéron de la Latin Library, j'ai mis 8 minutes. Vous pouvez, si vous voulez que ce soit moins long, créer un dossier vous-même, mettre les textes que vous voulez dedans, et le compresser ensuite."
      ],
      "metadata": {
        "id": "VsFp8bq8ecOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resultats = []\n",
        "for root_dir, subdirs, files in os.walk(base_dir):\n",
        "    for fname in files:\n",
        "        lower = fname.lower()\n",
        "        fpath = os.path.join(root_dir, fname)\n",
        "\n",
        "        texte = None\n",
        "\n",
        "        if lower.endswith(\".txt\"):\n",
        "            try:\n",
        "                with open(fpath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                    texte = f.read()\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur lecture TXT pour {fpath}: {e}\")\n",
        "\n",
        "        elif lower.endswith(\".xml\") and \"eng\" not in lower:\n",
        "            texte = extract_body_text_from_xml(fpath)\n",
        "\n",
        "        if not texte:\n",
        "            continue\n",
        "        titre = os.path.relpath(fpath, base_dir)\n",
        "        total_lemmes, uniques_lemmes = fast_vocab_stats(fpath, texte, nlp_stanza)\n",
        "        resultats.append((titre, total_lemmes, uniques_lemmes))"
      ],
      "metadata": {
        "id": "qVkdaN-9aUjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not resultats:\n",
        "    raise ValueError(\"Aucun fichier .txt ou .xml (sans 'eng' dans le nom) n'a été trouvé dans le dossier.\")"
      ],
      "metadata": {
        "id": "BASq_BpBcwbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBImbR491ocD"
      },
      "outputs": [],
      "source": [
        "fichiers, totals, uniques = zip(*resultats)\n",
        "index = range(len(fichiers))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "8bqJ7d8EeYkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.bar(index, totals, label='Total de lemmes')\n",
        "plt.bar(index, uniques, label='Lemmes uniques', alpha=0.7)\n",
        "plt.xlabel('Textes')\n",
        "plt.ylabel('Nombre de lemmes')\n",
        "\n",
        "plt.xticks(index, fichiers, rotation=45, ha='right', fontsize=8)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.legend(fontsize=10)\n",
        "plt.title('Richesse du vocabulaire par texte')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"richesse_vocabulaire.svg\", format=\"svg\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e6IZd7i6c6l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_auteur(path):\n",
        "    import os\n",
        "    base = os.path.basename(path)\n",
        "    if base.startswith(\"serv.\"):\n",
        "        return \"servius\"\n",
        "    return path.split(os.sep)[0]"
      ],
      "metadata": {
        "id": "YFgGUNjz25bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SVpk5Ra3P8N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame(resultats, columns=[\"texte\", \"total\", \"uniques\"])\n",
        "df[\"ttr\"] = df[\"uniques\"] / df[\"total\"]\n",
        "df[\"log_total\"] = np.log10(df[\"total\"])\n",
        "df[\"log_uniques\"] = np.log10(df[\"uniques\"])\n",
        "df[\"auteur\"] = df[\"texte\"].apply(lambda p: p.split(os.sep)[0])\n",
        "df[\"auteur\"] = df[\"texte\"].apply(detect_auteur)\n",
        "df[\"auteur\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"log_total\"] = np.log10(df[\"total\"])\n",
        "df[\"log_uniques\"] = np.log10(df[\"uniques\"])\n",
        "\n",
        "beta, logK = np.polyfit(df[\"log_total\"], df[\"log_uniques\"], 1)\n",
        "print(\"beta =\", beta, \"logK =\", logK)"
      ],
      "metadata": {
        "id": "Ct9nBfiq4iLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"richesse_norm\"] = df[\"uniques\"] / (df[\"total\"] ** beta)\n",
        "df[\"richesse_norm\"] = df[\"richesse_norm\"] / df[\"richesse_norm\"].mean()"
      ],
      "metadata": {
        "id": "dXcYLiwc4ii1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(9, 6))\n",
        "\n",
        "for auteur in df[\"auteur\"].unique():\n",
        "    sub = df[df[\"auteur\"] == auteur]\n",
        "    plt.scatter(sub[\"total\"], sub[\"richesse_norm\"], label=auteur, alpha=0.8)\n",
        "\n",
        "plt.xscale(\"log\")\n",
        "plt.axhline(1.0, linestyle=\"--\")\n",
        "\n",
        "plt.xlabel(\"Total de lemmes (échelle log)\")\n",
        "plt.ylabel(\"Richesse lexicale normalisée\\n(1 = attendue pour cette longueur)\")\n",
        "plt.title(\"Richesse lexicale corrigée de la longueur, par texte\")\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    x = row[\"total\"]\n",
        "    y = row[\"richesse_norm\"]\n",
        "    label = os.path.basename(row[\"texte\"])\n",
        "    plt.annotate(\n",
        "        label,\n",
        "        (x, y),\n",
        "        textcoords=\"offset points\",\n",
        "        xytext=(5, 3),\n",
        "        fontsize=8,\n",
        "    )\n",
        "\n",
        "plt.legend(title=\"Auteur\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wE23K-Lr4obf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auteurs = df[\"auteur\"].unique()\n",
        "data = [df[df[\"auteur\"] == a][\"richesse_norm\"] for a in auteurs]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.boxplot(data, labels=auteurs)\n",
        "\n",
        "for i, a in enumerate(auteurs, start=1):\n",
        "    sub = df[df[\"auteur\"] == a]\n",
        "    x_jitter = np.random.normal(loc=i, scale=0.03, size=len(sub))\n",
        "    plt.scatter(x_jitter, sub[\"richesse_norm\"], alpha=0.6)\n",
        "\n",
        "plt.axhline(1.0, linestyle=\"--\")\n",
        "\n",
        "plt.ylabel(\"Richesse lexicale normalisée\\n(1 = moyenne du corpus)\")\n",
        "plt.xlabel(\"Auteur\")\n",
        "plt.title(\"Distribution de la richesse lexicale normalisée par auteur\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x22xmbPP4q_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Axe Y: richesse_norm a été construite pour que :\n",
        "\n",
        "= à 1, richesse moyenne du corpus, à longueur donnée.\n",
        "\n",
        "supérieur à 1, texte plus riche que la moyenne pour sa longueur.\n",
        "\n",
        "< 1, texte moins riche que la moyenne pour sa longueur.\n",
        "\n",
        "La ligne pointillée est le niveau moyen (1).\n",
        "\n",
        "Chaque boîte = la distribution des textes d'un auteur :\n",
        "\n",
        "\n",
        "\n",
        "*   trait orange = médiane,\n",
        "*   boîte = quartiles (50 % des textes),\n",
        "*   moustaches + points = dispersion et éventuels intrus."
      ],
      "metadata": {
        "id": "1ecI7ioB6eho"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sQuwtakq6BP1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyN5Z13TT/+PA735Ep2m13iH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}